{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 1. Project Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.1 Project Title\n",
    "\n",
    "Beer reviews data analysis using machine learning, exploratory data visualization and analysis techniques.\n",
    "\n",
    "## 1.2. Background Information\n",
    "This project focuses on optimizing the brewing process for a craft beer brewery, using a dataset covering January 2020 to January 2024. By analyzing brewing parameters such as fermentation time, temperature, and ingredient ratios, our goal is to establish correlations between brewing techniques and beer quality and identify optimal brewing conditions that improve product quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Reading the Data Source\n",
    "\n",
    "For this project, we are using a dataset in a csv file. Given our collaborative work environment and the use of GitHub for version control, we've chosen to host this file online to ensure all team members can access and load the data directly into the Jupyter notebook. \n",
    "\n",
    "The .csv file is hosted on Dropbox, on the following link:: \n",
    "\n",
    "[Brewery Data Link](https://www.kaggle.com/datasets/rdoume/beerreviews/data?select=beer_reviews.csv)\n",
    "The dataset variables description is:\n",
    "\n",
    "- Brewery_id - Unique identifier for the brewery.\n",
    "- Brewery_name -  Name of the brewery.\n",
    "- Review_time - Timestamp of the review in Unix time format.\n",
    "- Review_overall - Overall rating given by the reviewer.\n",
    "- Review_aroma - Rating for the aroma of the beer.\n",
    "- Review_appearance - Rating for the appearance of the beer.\n",
    "- Review_profilename - Profile name of the reviewer.\n",
    "- Beer_style - Style of the beer being reviewed.\n",
    "- Review_palate - Rating for the palate of the beer.\n",
    "- Review_taste - Rating for the taste of the beer.\n",
    "- Beer_name - Beer name.\n",
    "- Beer_abv - Alcohol By Volume (ABV) of the beer.\n",
    "- Beer_beerid - Unique identifier for the beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Library Imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go \n",
    "import plotly.subplots as sp\n",
    "import plotly.figure_factory as ff\n",
    "import os\n",
    "import kagglehub\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"rdoume/beerreviews\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_path = os.path.join(path, \"beer_reviews.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "print(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. Data Cleaning and Preprocessing\n",
    "\n",
    "During this phase we will prepare the dataset for analysis and modeling. First, the structure of the dataset will be inspected, in order to understand the structure and its features along with their data types. After that, missing values will be handled, either by imputing or deleting them. Finally, data transformation is applied to convert some variables to a more suited scale.\n",
    "\n",
    "## 3.1. Data Structure\n",
    "\n",
    "We use info() to check the structure of our data. We will look into the data type of each column and the number of data points and variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtain data structure\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Doing an initial analysis to the structure of the data, we can observe the following details:\n",
    "\n",
    "- The dataset contains 1,586,614 observations, with 13 distinct variables.\n",
    "- The memory usage is approximately 157.4 MB.\n",
    "- The 'brewery_id' and 'brewery_name' variables represent the brewery's unique identifier and name. 'Brewery_id' could be converted into a categorical variable if the IDs represent different categories or groups of breweries.\n",
    "- Ratings for aroma, appearance, palate, and taste ('review_aroma', 'review_appearance', 'review_palate', 'review_taste') are provided. Analyze the distribution of these ratings and explore potential relationships between them.\n",
    "- Explore the distribution of beer styles 'beer_style' to identify the most common styles and analyze how they are rated.\n",
    "- Analyze the distribution of 'beer_abv' and explore how it correlates with overall ratings or specific aspects of the reviews.\n",
    "- There are missing values in 'brewery_name', 'review_profilename', and 'beer_abv'. Determine if there are patterns or reasons for missing data.\n",
    "- There is potential relationships between different variables that could be explored. 'review_taste' or 'review_overall' might be influenced by 'review_aroma', 'review_appearance', or 'review_palate'.\n",
    "- Exploring trends in the number of reviews over time 'review_time' will help us understand if there are seasonality or significant changes in reviewing behavior.\n",
    "\n",
    "These observations are just a starting point, and further exploration of the data might reveal additional information and considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Summary Statistics\n",
    "\n",
    "Now during this step, we intend to modify the data to better suit our analysis and the predicting modeling building. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Statistics Analysis\n",
    "\n",
    "The summary statistics of the dataset provide valuable information into the distribution and characteristics of beer reviews:\n",
    "\n",
    "- **Count:** There are a total of 1,586,614 reviews in the dataset. However, the `beer_abv` column has fewer non-null values (1,518,829), indicating some missing data in this column.\n",
    "\n",
    "- **Review Ratings (Overall, Aroma, Appearance, Palate, Taste):**\n",
    "    - The average ratings for overall, aroma, appearance, palate, and taste are all above 3.5, suggesting a generally positive sentiment in the reviews.\n",
    "    - The minimum ratings for overall, aroma, appearance, palate, and taste are 0, 1, 0, 1, and 1, respectively. The presence of 0 in overall and appearance ratings may indicate some outlier or erroneous entries.\n",
    "    - The maximum rating for all these categories is 5, which is consistent with a typical rating scale.\n",
    "\n",
    "- **Alcohol by Volume (ABV):**\n",
    "    - The average ABV is 7.04%, with a standard deviation of 2.32%, indicating a moderate variation in the alcohol content of the beers reviewed.\n",
    "    - The minimum ABV is 0.01%, which is unusually low for a beer. This could be an outlier or a special case (e.g., non-alcoholic beer).\n",
    "    - The maximum ABV is 57.7%, which is exceptionally high for a beer and likely represents a special or extreme brew.\n",
    "\n",
    "- **Brewery and Beer IDs:**\n",
    "    - The `brewery_id` and `beer_beerid` columns have a wide range of values, indicating a diverse set of breweries and beers in the dataset.\n",
    "\n",
    "These summary statistics provide an understanding of the dataset's structure and the distribution of key variables. They also highlight potential areas for further investigation, such as examining the distribution of ABV values and investigating the reasons behind the low and high extremes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3. Check for Duplicate values\n",
    "\n",
    "Let's investigate if there are any duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "\n",
    "# Count the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There are no duplicate values so we do not need to delete any rows based on that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.4. Handling Missing Values\n",
    "\n",
    "Missing values need to be handled because most models do not handle them well. For missing values, the options are to:\n",
    "\n",
    "- Substitute them with statistical estimates such as the mean, median, mode, etc. \n",
    "- Eliminate them all together, if the amount of missing values is relatively low. \n",
    "\n",
    "We will do some analysis to see if there are missing values in our dataset, and if so, how to handle them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values in each column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "\n",
    "# Total number of missing values\n",
    "total_missing_values = missing_values.sum()\n",
    "print(f\"\\nTotal number of missing values in the dataset: {total_missing_values}\")\n",
    "\n",
    "# List of columns that contain at least one missing value\n",
    "columns_with_missing_values = missing_values[missing_values > 0].index.tolist()\n",
    "print(\"\\nColumns that contain at least one missing value:\")\n",
    "print(columns_with_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Our analysis of the missing values reveals the following:\n",
    "\n",
    "- The total number of missing values in the dataset is 68,148.\n",
    "- The columns that contain at least one missing value are 'brewery_name', 'review_profilename', and 'beer_abv'.\n",
    "- Given the wide range of values in the 'brewery_name' variable and the concern that imputing with the mode might not be representative, we have opted to remove the rows with missing values in this column.\n",
    "- The 'review_profilename' represents categorical data and there are 348 missing values, we have decided to impute the most appearing user profile name.\n",
    "- With a significant number of missing values in 'beer_abv,' we recognize the importance of retaining data while providing a reasonable estimate. To accomplish this, we have chosen to impute the missing values using the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values in 'brewery_name'\n",
    "df_cleaned = df.dropna(subset=['brewery_name']).copy()\n",
    "\n",
    "# Impute missing values in 'review_profilenam\n",
    "mode_profilename = df_cleaned['review_profilename'].mode()[0]\n",
    "df_cleaned['review_profilename'] = df_cleaned['review_profilename'].fillna(mode_profilename)\n",
    "\n",
    "# Impute missing values in 'beer_abv' with the mean\n",
    "df_cleaned['beer_abv'] = df_cleaned['beer_abv'].fillna(df_cleaned['beer_abv'].mean())\n",
    "\n",
    "# Display the final cleaned dataset\n",
    "print(\"Final Cleaned Dataset:\")\n",
    "print(df_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the final cleaned dataset\n",
    "print(\"\\nMissing values in the final cleaned dataset:\")\n",
    "missing_values_cleaned = df_cleaned.isnull().sum()\n",
    "print(missing_values_cleaned)\n",
    "\n",
    "# Total number of missing values in the final cleaned dataset\n",
    "total_missing_values_cleaned = missing_values_cleaned.sum()\n",
    "print(f\"\\nTotal number of missing values in the final cleaned dataset: {total_missing_values_cleaned}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We ensured there are no missing values; this indicates that the data is now complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Data Transformation\n",
    "\n",
    "## 4.1. Data Type Conversion\n",
    "\n",
    "Before going into further analysis, it is important that each column in our dataset is of the appropriate data type. This step is important for several reasons:\n",
    "\n",
    "- **Datetime Conversion:** The `review_time` column, currently in UNIX timestamp format, will be converted to a datetime format for easier manipulation and interpretation of time-based data.\n",
    "- **Floating-Point Conversion:** Numerical columns such as `review_overall`, `review_aroma`, and `beer_abv` will be converted to float data types to ensure precision in numerical operations.\n",
    "- **Categorical Conversion:** Columns representing categorical data, such as `brewery_name`, `beer_style`, and `beer_beerid`, will be converted to the `category` data type. This conversion is beneficial for memory efficiency and computational performance when dealing with categorical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'review_time' to datetime format\n",
    "df_cleaned['review_time'] = pd.to_datetime(df_cleaned['review_time'], unit='s')\n",
    "\n",
    "# Convert numerical columns to float\n",
    "numerical_cols = ['review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste', 'beer_abv']\n",
    "df_cleaned[numerical_cols] = df_cleaned[numerical_cols].astype(float)\n",
    "\n",
    "# Convert categorical variables to 'category' data type\n",
    "categorical_cols = ['brewery_name', 'beer_style', 'beer_name', 'brewery_id', 'beer_beerid']\n",
    "df_cleaned[categorical_cols] = df_cleaned[categorical_cols].astype('category')\n",
    "\n",
    "# Display the updated data types\n",
    "print(df_cleaned.dtypes)\n",
    "df_cleaned.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Conversion Summary\n",
    "\n",
    "After converting the data types:\n",
    "\n",
    "- The `review_time` column has been successfully converted to a datetime format, facilitating time-based analysis.\n",
    "- Numerical columns like `review_overall` and `beer_abv` are now in float format, ensuring accurate numerical computations.\n",
    "- Categorical columns such as `brewery_name` and `beer_style` have been converted to the `category` data type, enhancing memory efficiency and computational speed for categorical operations.\n",
    "- The updated data types are displayed above, confirming the successful conversions and preparing our dataset for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Outliers Analysis\n",
    "\n",
    "Given that our reviews should range from 1 to 5, outliers would be any values below 1 or above 5. These could be due to data entry errors or issues with data collection. Let's identify and handle these outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define review score columns\n",
    "review_score_columns = ['review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste']\n",
    "\n",
    "# Initialize dictionary to hold the counts of outliers\n",
    "outliers_dict = {}\n",
    "\n",
    "# Count outliers for each review score column\n",
    "for column in review_score_columns:\n",
    "    outliers_below = df_cleaned[df_cleaned[column] < 1].shape[0]\n",
    "    outliers_above = df_cleaned[df_cleaned[column] > 5].shape[0]\n",
    "    outliers_dict[column] = {'below_1': outliers_below, 'above_5': outliers_above}\n",
    "\n",
    "# Display the results\n",
    "for column, counts in outliers_dict.items():\n",
    "    print(f\"{column} - Outliers below 1: {counts['below_1']}, Outliers above 5: {counts['above_5']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Boxplot Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical columns for visualization\n",
    "numerical_cols = ['review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste', 'beer_abv']\n",
    "\n",
    "# Create boxplots for each numerical column\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    plt.subplot(2, 3, i)\n",
    "    df_cleaned.boxplot(column=col)\n",
    "    plt.title(col)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removal of Outliers Outside the Valid Score Range\n",
    "\n",
    "Upon closer examination of our review score data, we've identified a small number of entries that have `review_overall` and `review_appearance` scores below the valid range of 1 to 5. Since our rating system does not accommodate scores below 1, these entries are likely to be data entry errors and will be removed to maintain the integrity of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the outliers with review scores below 1\n",
    "df_cleaned = df_cleaned[(df_cleaned['review_overall'] >= 1) & (df_cleaned['review_appearance'] >= 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define review score columns\n",
    "review_score_columns = ['review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste']\n",
    "\n",
    "# Initialize dictionary to hold the counts of outliers\n",
    "outliers_dict = {}\n",
    "\n",
    "# Count outliers for each review score column\n",
    "for column in review_score_columns:\n",
    "    outliers_below = df_cleaned[df_cleaned[column] < 1].shape[0]\n",
    "    outliers_above = df_cleaned[df_cleaned[column] > 5].shape[0]\n",
    "    outliers_dict[column] = {'below_1': outliers_below, 'above_5': outliers_above}\n",
    "\n",
    "# Display the results\n",
    "for column, counts in outliers_dict.items():\n",
    "    print(f\"{column} - Outliers below 1: {counts['below_1']}, Outliers above 5: {counts['above_5']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entries with invalid review scores have been removed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate the High ABV Values:\n",
    "High alcohol by volume (ABV) in beers can be quite normal, especially for certain styles of beer like barleywines, imperial stouts, and others that are known to have higher ABV percentages. However, exceptionally high values (e.g., significantly above 20%) may be erroneous or represent a very niche type of beer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate high ABV values by checking the beer names and styles\n",
    "high_abv_beers = df_cleaned[df_cleaned['beer_abv'] > 20]  # Example cutoff\n",
    "high_abv_beers[['beer_name', 'beer_style', 'beer_abv']].sort_values(by='beer_abv', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inclusion of High ABV Beers in the Dataset\n",
    "\n",
    "After a thorough verification using the Beer Advocate website, we have confirmed that the exceptionally high ABV beers in our dataset are genuine. Beers like \"SchorschbrÃ¤u Schorschbock 57%\" and \"Sink The Bismarck!\" with ABV levels above 20% are indeed specialty beers with significantly higher alcohol content than typical beers.\n",
    "\n",
    "Given that these are legitimate products, we have decided to include these high ABV beers in our dataset. This will allow our analysis to reflect the full spectrum of beer varieties, including those that are at the extreme end of the ABV scale. These entries provide valuable insight into the diversity of the beer market and consumer preferences for high-strength beers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.1. Distributions of Key Variables\n",
    "\n",
    "### Review Score Distributions\n",
    "\n",
    "We will begin our exploratory data analysis by examining the distributions of the review scores. These scores provide insights into consumer preferences and beer quality as perceived by the reviewers. We want to understand the central tendencies, dispersions, and the overall shape of the distribution of review scores such as 'review_overall', 'review_aroma', 'review_appearance', 'review_palate', and 'review_taste'.\n",
    "\n",
    "To visualize these distributions, we will use histograms which offer a clear view of the frequency of different rating scores. We will also calculate skewness and kurtosis for these review scores to identify any asymmetry and the tailedness of the distributions, respectively. It's important to investigate these aspects as they can influence the interpretation of the average scores and the general user sentiment towards the beers reviewed.\n",
    "\n",
    "Additionally, understanding the most reviewed beer styles can help identify consumer trends and popular types of beer, which may be beneficial for market analysis and product development strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure layout dimensions \n",
    "fig_width = 1200  \n",
    "fig_height = 600 \n",
    "\n",
    "# Create histograms for review scores \n",
    "review_score_columns = ['review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste']\n",
    "\n",
    "# Create figure with subplots\n",
    "fig = sp.make_subplots(rows=2, cols=3, subplot_titles=review_score_columns) \n",
    "\n",
    "for index, column in enumerate(review_score_columns, 1):\n",
    "    # Calculate row and column positions for subplot layout\n",
    "    row = (index - 1) // 3 + 1  \n",
    "    col = (index - 1) % 3 + 1 \n",
    "\n",
    "    # Create a histogram trace \n",
    "    hist_trace = go.Histogram(\n",
    "        x=df_cleaned[column], \n",
    "        nbinsx=20,\n",
    "        name=column.capitalize()\n",
    "    )\n",
    "\n",
    "    # Add the trace to the figure with subplot specification\n",
    "    fig.add_trace(hist_trace, row=row, col=col)\n",
    "\n",
    "# Update figure layout\n",
    "fig.update_layout(\n",
    "    title=\"Distributions of Review Scores\",\n",
    "    width=fig_width, \n",
    "    height=fig_height,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Calculate skewness and kurtosis for review scores\n",
    "print(\"Skewness of review scores:\")\n",
    "print(df_cleaned[review_score_columns].skew())\n",
    "print(\"\\nKurtosis of review scores:\")\n",
    "print(df_cleaned[review_score_columns].kurt())\n",
    "\n",
    "# Calculate the top 20 most frequent beer styles\n",
    "top_n = 20\n",
    "top_20_styles = df_cleaned['beer_style'].value_counts().head(top_n)\n",
    "\n",
    "# Create a DataFrame (Plotly Express works well with DataFrames)\n",
    "df_top_styles = pd.DataFrame({'Beer Style': top_20_styles.index, \n",
    "                              'Number of Reviews': top_20_styles.values})\n",
    "\n",
    "# Create the bar chart using Plotly Express\n",
    "fig = px.bar(df_top_styles, \n",
    "             x='Number of Reviews', \n",
    "             y='Beer Style',\n",
    "             title='Top 20 Most Reviewed Beer Styles',\n",
    "             orientation='h',  \n",
    "             color='Beer Style',  \n",
    "             )\n",
    "\n",
    "# Customize appearance\n",
    "fig.update_layout(xaxis_title='Number of Reviews',\n",
    "                  yaxis_title='',  \n",
    "                  )\n",
    "\n",
    "# Show the chart\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results: Review Score Distributions\n",
    "\n",
    "The histograms for each review score category show a clear preference towards higher ratings, indicating a general positivity in reviewer feedback. However, the calculated skewness values are negative across all review scores, suggesting that the distribution tails are longer on the lower end. This means that there are relatively fewer low scores, but these can significantly differ from the average ratings.\n",
    "\n",
    "Similarly, the positive kurtosis values for all review scores indicate that the distributions have heavier tails and sharper peaks than the normal distribution. This reflects a high level of agreement among reviewers on certain ratings, with fewer moderate opinions.\n",
    "\n",
    "In the context of beer styles, the horizontal bar plot highlights the top 20 most reviewed beer styles, with 'American IPA' and 'American Double / Imperial IPA' being the most frequently reviewed. This suggests a high popularity and consumer interest in these categories, potentially offering valuable data to breweries looking to cater to market demands.\n",
    "\n",
    "Overall, the review score distributions and the frequency of reviews across different beer styles paint a comprehensive picture of the current landscape of beer reviews on the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution Analysis of Alcohol by Volume (ABV)\n",
    "\n",
    "The Alcohol by Volume (ABV) percentage in beer gives us an understanding of the strength of the beer. The ABV can vary significantly across different beer styles, influencing the flavor, body, and overall drinking experience. In this part of the analysis, we will create a histogram to visualize the distribution of ABV percentages across our dataset.\n",
    "\n",
    "By examining the distribution, we identify the most common strength of beers reviewed and determine if our dataset aligns with general beer style ABV expectations. High concentrations in certain ABV ranges could indicate popular market trends or reviewer preferences.\n",
    "\n",
    "This analysis could help breweries and beer enthusiasts alike to determine the popularity of different beer strengths and styles. It also provides an understanding into how ABV correlates with taste preferences and reviewing behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the ABV using Plotly Express\n",
    "fig = px.histogram(\n",
    "    df_cleaned, \n",
    "    x='beer_abv', \n",
    "    nbins=50, \n",
    "    marginal='violin',  \n",
    "    color_discrete_sequence=['blue'] \n",
    ")\n",
    "\n",
    "# Update layout for a more appealing look\n",
    "fig.update_layout(\n",
    "    title='Distribution of Alcohol by Volume (ABV)',\n",
    "    xaxis_title='ABV (%)',\n",
    "    yaxis_title='Frequency'\n",
    ")\n",
    "\n",
    "# Show the interactive plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings: ABV Distribution Analysis\n",
    "\n",
    "The histogram of the ABV distribution reveals a concentration of beers in the lower to mid-range of alcohol content, which is typical for many popular beer styles. The presence of a long tail towards the higher ABV percentages suggests that there are fewer but still significant offerings of stronger beers. This could reflect a niche market for high-strength beers, such as Imperial Stouts or Barleywines.\n",
    "\n",
    "The ABV distribution generally aligns with known beer style ABV ranges. The peak of the distribution occurs around what is standard for many ales and lagers, indicating that these styles are well-represented in the dataset. High-ABV beers are less common, but the variety within this range could speak to the diversity of the beer market and the experimentation among craft brewers.\n",
    "\n",
    "In summary, the ABV analysis shows the wide range of beer strengths available and reviewed, from light and sessionable to strong and bold. This variability supports the notion of a diverse and evolving beer culture where consumers have a broad array of choices to suit their taste and preferences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Relationships Between Variables\n",
    "\n",
    "### Exploring Relationships Between Variables\n",
    "\n",
    "Understanding the relationships between different variables in our dataset can tell how certain aspects of beers are related to each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for the pair plot\n",
    "columns = ['review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste', 'beer_abv']\n",
    "\n",
    "# Create a pair plot\n",
    "pair_plot = sns.pairplot(df_cleaned[columns])\n",
    "\n",
    "# Adjust the size of the pair plot if necessary\n",
    "pair_plot.fig.set_size_inches(15,15)\n",
    "\n",
    "# Add a main title to the pair plot\n",
    "plt.subplots_adjust(top=0.95)\n",
    "pair_plot.fig.suptitle('Pairwise Relationships Between Review Scores and ABV', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Plot Analysis Summary\n",
    "\n",
    "The pair plot provided insights into the relationships between review scores for overall quality, aroma, appearance, palate, taste, and the Alcohol by Volume (ABV) percentage. Here's what we can deduce from the visualization:\n",
    "\n",
    "**Consistent Review Scores**\n",
    "- There is a **positive correlation** between the different review scores. Higher scores in one sensory attribute tend to coincide with higher scores in others, indicating **consistency** in reviewers' perceptions.\n",
    "\n",
    "**Positive Reviews**\n",
    "- Scores predominantly cluster around the 3.5 to 4.5 range, suggesting that the majority of reviews are **favorable**. The histograms show a skew towards higher ratings, with fewer low scores.\n",
    "\n",
    "**ABV Distribution**\n",
    "- The `beer_abv` content is mostly concentrated in the lower range (approximately 5% to 10%), which aligns with the ABV of **common beer styles**. High ABV beers are present but significantly less common.\n",
    "\n",
    "**### ABV vs Review Scores**\n",
    "- The relationship between ABV and review scores does not show a distinct pattern, suggesting that beer strength does not solely influence its perceived quality. The diversity in scoring across ABV levels indicates that **other factors** may have a more substantial impact on reviews.\n",
    "\n",
    "**Presence of Outliers** \n",
    "- Outliers in the `beer_abv` suggest the existence of specialty beers with much higher than average alcohol content. \n",
    "\n",
    "**Review Scores Density**\n",
    "- The peaks in the distribution for each review score category could indicate a **tendency** among reviewers to prefer certain scores or might reflect a general consensus on beer quality.\n",
    "\n",
    "In conclusion, while the dataset reveals general positivity in beer reviews and consistent relationships between the different review scores, ABV does not emerge as a significant determinant of review outcomes. Instead, the analysis points towards the importance of other variables in influencing reviewer opinions, which could be explored in further studies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.3. Correlation Matrix\n",
    "\n",
    "We now focus on identifying the strength and direction of the relationships between different beer attributes. The correlation matrix is a powerful tool in this exploration, allowing us to visualize how closely linked certain characteristics, such as taste, aroma, and appearance, are to each other and to the alcohol by volume (ABV) percentage.\n",
    "\n",
    "We would like to discover if there are any patterns that indicate, for instance, whether beers with higher ABV tend to receive better or worse reviews. \n",
    "\n",
    "The following code block generates an annotated heatmap to visualize these correlations. It offers a snapshot of the interdependencies within our dataset, providing clarity on which attributes move together and which diverge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only numeric columns for correlation matrix\n",
    "numeric_df = df_cleaned.select_dtypes(include=[np.number])\n",
    "\n",
    "# Recalculate the correlation matrix\n",
    "corr = numeric_df.corr()\n",
    "\n",
    "# Generate the plot\n",
    "fig = ff.create_annotated_heatmap(\n",
    "    z=corr.to_numpy(),\n",
    "    x=corr.columns.tolist(),\n",
    "    y=corr.index.tolist(),\n",
    "    annotation_text=corr.round(2).astype(str).to_numpy(),\n",
    "    colorscale='Viridis',\n",
    "    showscale=True\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Correlation Matrix of Beer Review Scores',\n",
    "    xaxis={'title': 'Variables'},\n",
    "    yaxis={'title': 'Variables'},\n",
    "    autosize=False,  \n",
    "    width=1800,       \n",
    "    height=600       \n",
    ")\n",
    "\n",
    "# Show the interactive figure\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Matrix Analysis\n",
    "\n",
    "The resulting heatmap from the correlation matrix provides us with several noteworthy observations:\n",
    "\n",
    "- There's a strong positive correlation between the review scores for taste, aroma, palate, and appearance, suggesting that reviewers who enjoy the taste of a beer often find the aroma, palate, and appearance to be pleasant as well. It indicates a general consistency in the perception of beer quality across different sensory dimensions.\n",
    "- There's a very strong correlation between the overall review score and the scores for taste and palate. This suggests that taste and palat are the mot influencial factors.\n",
    "- The overall review score also shows a strong correlation with the aroma score, highlighting the importance of scent in the overall experience of the beer.\n",
    "- There is a moderately strong correlation between the overall review score and the appearance score. This reflects that visual appeal does play a role in the overall impression but perhaps not as much as taste or aroma.\n",
    "- The Alcohol by Volume (ABV) percentage shows a weaker correlation with the review scores, which could imply that the strength of the beer does not consistently influence how it is rated in terms of taste, aroma, etc.\n",
    "- Interestingly, the correlation between ABV and the other attributes is not negative, indicating that stronger beers are not necessarily rated poorly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Time Series Analysis of Beer Ratings and ABV\n",
    "\n",
    "Now we do a time series analysis of beer ratings and Alcohol by Volume (ABV) percentages to uncover patterns that may inform brewers and consumers alike about shifts in beer preferences and brewing strengths.\n",
    "\n",
    "- The average overall rating trends, which may reflect changes in consumer satisfaction or review behaviors over time.\n",
    "- The average ABV trends, indicating if there's been a shift toward stronger or milder beers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to exclude non-numeric columns before grouping\n",
    "numeric_columns = df_cleaned.select_dtypes(include=[np.number])\n",
    "df_grouped = numeric_columns.groupby(df_cleaned['review_time'].dt.to_period('M')).mean()  # Group by month\n",
    "\n",
    "# Reset index to get 'review_time' as a column\n",
    "df_grouped.reset_index(inplace=True)\n",
    "\n",
    "# Convert the period column to datetime to make sure Matplotlib can handle it\n",
    "df_grouped['review_time'] = df_grouped['review_time'].dt.to_timestamp()\n",
    "\n",
    "# Create line plots\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# Plot average overall rating\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(df_grouped['review_time'], df_grouped['review_overall'], label='Average Overall Rating')\n",
    "plt.title('Average Ratings and ABV Over Time')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.legend()\n",
    "\n",
    "# Plot average ABV\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(df_grouped['review_time'], df_grouped['beer_abv'], label='Average ABV', color='orange')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Average ABV (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Show plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of Trends in Beer Ratings and ABV\n",
    "\n",
    "The time series visualizations reveal intriguing patterns in beer ratings and ABV from 1996 to 2012:\n",
    "\n",
    "**Average Overall Rating**\n",
    "- The graph indicates a gradual stabilization of beer ratings over time. After an initial period of volatility, the ratings converge to a narrower band, suggesting a maturing market where consumers' expectations are increasingly met by the brewers' offerings.\n",
    "\n",
    "**Average ABV**\n",
    "- Contrasting the stability in beer ratings, the ABV shows a gradual increase. This trend could be reflective of a growing consumer interest in stronger beers, perhaps in line with the rise of craft breweries that often experiment with higher ABV ranges.\n",
    "\n",
    "**Conclusion**\n",
    "- These observations may suggest a market that's settling in terms of quality expectations while simultaneously developing a taste for potency in beer profiles. Brewers can infer that there is a potential market segment leaning towards beers with higher ABV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Breweries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Calculate the top 10 most reviewed breweries\n",
    "top_n = 10\n",
    "top_10_breweries = df_cleaned['brewery_name'].value_counts().head(top_n)\n",
    "\n",
    "# Create a DataFrame suitable for Plotly Express\n",
    "df_top_breweries = pd.DataFrame({'Brewery Name': top_10_breweries.index, \n",
    "                                 'Number of Reviews': top_10_breweries.values})\n",
    "\n",
    "# Create the bar chart using Plotly Express\n",
    "fig = px.bar(df_top_breweries, \n",
    "             x='Number of Reviews', \n",
    "             y='Brewery Name',\n",
    "             title='Top 10 Breweries by Review Count',\n",
    "             orientation='h',  # horizontal bar chart\n",
    "             color='Brewery Name',  # color code by brewery\n",
    "             )\n",
    "\n",
    "# Customize the chart's appearance\n",
    "fig.update_layout(\n",
    "    xaxis_title='Number of Reviews',\n",
    "    yaxis_title='',\n",
    "    yaxis=dict(autorange=\"reversed\"),  # to have the highest count on top\n",
    ")\n",
    "\n",
    "# Show the chart\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top Reviewers\n",
    "# Find the top 10 reviewers based on the number of reviews they've submitted\n",
    "top_reviewers = df_cleaned['review_profilename'].value_counts().head(10)\n",
    "top_reviewers_df = pd.DataFrame({'Reviewer': top_reviewers.index, 'Number of Reviews': top_reviewers.values})\n",
    "\n",
    "# Plotting the top reviewers with a bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Number of Reviews', y='Reviewer', data=top_reviewers_df, palette='viridis')\n",
    "plt.title('Top 10 Reviewers by Number of Reviews')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Reviewer')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Rating Patterns\n",
    "# Calculate the average rating for each reviewer and count how many ratings they've submitted\n",
    "reviewer_means = df_cleaned.groupby('review_profilename')['review_overall'].agg(['mean', 'count']).reset_index()\n",
    "# Filter out reviewers with a very low number of reviews to avoid skewing the results\n",
    "reviewer_means_filtered = reviewer_means[reviewer_means['count'] > 50]  # arbitrary threshold\n",
    "\n",
    "# Plotting the distribution of average ratings for reviewers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(reviewer_means_filtered['mean'], bins=30, kde=False)\n",
    "plt.title('Distribution of Average Ratings by Reviewers')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# You can also plot a scatter plot to visualize the relationship between the number of reviews and the average rating\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='count', y='mean', data=reviewer_means_filtered)\n",
    "plt.title('Number of Reviews vs Average Rating by Reviewers')\n",
    "plt.xlabel('Number of Reviews')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Top 10 Reviewers by number of reviews**\n",
    "\n",
    "This chart highlights a significant variation in activity levels among reviewers.  Some reviewers, like 'northyorksammy' are extremely prolific, while others contribute a smaller number of reviews. It's important to be aware that the most frequent reviewers might disproportionately influence the overall perception of beers in the dataset. Their preferences and biases could carry more weight.\n",
    "\n",
    "**Distribution of Average Ratings by Reviewers**\n",
    "\n",
    "- The distribution appears to be centered around positive ratings (between 3.5 and 4.0 stars), suggesting that the majority of reviewers have a favorable impression of the beers they are reviewing.\n",
    "\n",
    "- The distribution has a somewhat bell-shaped curve, with a peak in the middle and tapering tails on either side. This suggests that ratings are clustered around the average, with fewer outliers at the extremes (very low or very high ratings).\n",
    "\n",
    "- The prevalence of positive ratings might indicate that the beers are generally well-received by reviewers.\n",
    "\n",
    "**Number of Reviews vs Average Rating by Reviewers**\n",
    "- There appears to be a weak positive correlation between the number of reviews and the average rating. Beers with more reviews tend to have slightly higher average ratings.\n",
    "\n",
    "- The data points are scattered, indicating that the number of reviews doesn't solely determine a beer's average rating. There are beers with a high number of reviews that have lower average ratings, and vice versa.\n",
    "Possible Interpretations\n",
    "\n",
    "- The positive correlation might suggest that more popular beers (those with more reviews) tend to be better-rated on average. This could be because popular beers are more likely to be well-known and established brands that receive consistently positive reviews.\n",
    "\n",
    "- Another possibility is that reviewers are more likely to leave reviews for beers they have strong opinions about, either positive or negative. This could lead to higher average ratings for both highly-rated and poorly-rated beers with a lot of reviews, and lower average ratings for less well-known beers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to implement K-means clustering, to segment beer reviews based on their sensory attributes and alcohol by volume (ABV) content. By clustering similar beer reviews together, we aimed to uncover underlying patterns and groupings within the dataset that could provide valuable insights for brewers and consumers alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns for clustering\n",
    "clustering_columns = ['review_overall', 'review_aroma', 'review_appearance', 'review_palate', 'review_taste', 'beer_abv']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_cleaned[clustering_columns])\n",
    "\n",
    "# Define the range of clusters to test\n",
    "min_clusters = 2\n",
    "max_clusters = 10\n",
    "num_clusters_range = range(min_clusters, max_clusters + 1)\n",
    "\n",
    "# Calculate within-cluster sum of squares (WCSS) for each number of clusters\n",
    "wcss = []\n",
    "for num_clusters in num_clusters_range:\n",
    "    kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    kmeans.fit(scaled_data)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(num_clusters_range, wcss, marker='o', linestyle='-')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.xticks(num_clusters_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters (you can adjust this based on your requirements)\n",
    "num_clusters = 5\n",
    "\n",
    "# Initialize K-means model\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "\n",
    "# Fit the model to the scaled data\n",
    "kmeans.fit(scaled_data)\n",
    "\n",
    "# Get the cluster labels for each sample\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add cluster labels to the original dataframe\n",
    "df_cleaned['cluster'] = cluster_labels\n",
    "\n",
    "# Check the distribution of clusters\n",
    "print(\"Distribution of clusters:\")\n",
    "print(df_cleaned['cluster'].value_counts())\n",
    "\n",
    "# Analyze cluster centroids\n",
    "cluster_centroids = scaler.inverse_transform(kmeans.cluster_centers_)\n",
    "cluster_centroids_df = pd.DataFrame(cluster_centroids, columns=clustering_columns)\n",
    "cluster_centroids_df.index.name = 'Cluster'\n",
    "print(\"\\nCluster centroids:\")\n",
    "print(cluster_centroids_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA to reduce the dimensionality of the data to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame with the principal components and cluster labels\n",
    "cluster_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "cluster_df['cluster'] = kmeans.labels_\n",
    "\n",
    "# Plot the clusters in 2D space\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    plt.scatter(cluster_df.loc[cluster_df['cluster'] == cluster, 'PC1'],\n",
    "                cluster_df.loc[cluster_df['cluster'] == cluster, 'PC2'],\n",
    "                label=f'Cluster {cluster}',\n",
    "                alpha=0.5)\n",
    "plt.title('Clusters in 2D PCA Space')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.2 K-means 2nd option\n",
    "- Selecting Features for Clustering:\n",
    "    - We began by selecting the numerical features that will be used for clustering. These features include 'review_aroma', 'review_appearance', 'review_palate', 'review_taste', and 'beer_abv'. These features represent different aspects of beer reviews and characteristics.\n",
    "- Scaling Numerical Features:\n",
    "    - Before applying K-Means clustering, it is important to scale the numerical features to ensure that each feature contributes equally to the clustering process. We used the StandardScaler to scale the numerical features to have zero mean and unit variance.\n",
    "- Selecting the Number of Clusters (k):\n",
    "    - We determined the number of clusters (k) to use in the K-Means algorithm. In this case, we chose to use 5 clusters, but the number of clusters can be adjusted based on domain knowledge or using techniques such as the elbow method or silhouette score.\n",
    "- Performing K-Means Clustering:\n",
    "    - We applied the K-Means clustering algorithm to the scaled numerical features. K-Means partitions the dataset into k clusters based on the similarity of data points. Each cluster is represented by its centroid, and data points are assigned to the nearest centroid.\n",
    "- Adding Cluster Labels to the DataFrame:\n",
    "    - After clustering, we added the cluster labels assigned by the K-Means algorithm to the original DataFrame. These cluster labels indicate which cluster each data point belongs to and will be used for further analysis and interpretation.\n",
    "- Visualizing Clusters Using PCA:\n",
    "\n",
    "    - We used Principal Component Analysis (PCA) for dimensionality reduction and visualization of the clusters. PCA transforms the original data into a lower-dimensional space while preserving the variance of the data. We visualized the clusters in a 2D scatter plot using the first two principal components to understand the distribution of data points in the reduced space.\n",
    "- Visualizing Clusters in 3D:\n",
    "    - Additionally, we performed PCA with three components to visualize the clusters in a 3D scatter plot. This provides a more comprehensive view of the clusters, allowing us to explore the data in three dimensions.\n",
    "    \n",
    "By completing this clustering analysis, we aim to uncover underlying patterns or structures in the beer review dataset. These insights can be valuable for various applications, such as targeted marketing, product recommendations, or customer segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selecting features for clustering (adjust as needed)\n",
    "numerical_features = ['review_aroma', 'review_appearance', 'review_palate', 'review_taste', 'beer_abv']\n",
    "\n",
    "# Extracting numerical feature values\n",
    "X = df_cleaned[numerical_features].values\n",
    "\n",
    "# Scaling the numerical features\n",
    "scaler2 = StandardScaler()\n",
    "X_scaled = scaler2.fit_transform(X)\n",
    "\n",
    "# Determine the optimal number of clusters using the elbow method\n",
    "inertia = []\n",
    "for n_clusters in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o', linestyle='-')\n",
    "plt.title('Elbow Method for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Selecting the number of clusters\n",
    "n_clusters = 5\n",
    "\n",
    "# Performing k-means clustering\n",
    "kmeans2 = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans2.fit(X_scaled)\n",
    "\n",
    "# Adding cluster labels to the DataFrame\n",
    "df_cleaned['cluster_label2'] = kmeans2.labels_\n",
    "\n",
    "# Visualizing the clusters using PCA for dimensionality reduction\n",
    "pca1 = PCA(n_components=2)\n",
    "X_pca = pca1.fit_transform(X_scaled)\n",
    "\n",
    "pca2 = PCA(n_components=3)\n",
    "X_pca2 = pca2.fit_transform(X_scaled)\n",
    "\n",
    "# Plotting the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster_label2 in range(n_clusters):\n",
    "    plt.scatter(X_pca[kmeans.labels_ == cluster_label2, 0],\n",
    "                X_pca[kmeans.labels_ == cluster_label2, 1],\n",
    "                label=f'Cluster {cluster_label2 + 1}')\n",
    "plt.title('Clusters Visualized Using PCA')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Adding PCA components to the DataFrame\n",
    "df_cleaned['PCA1'] = X_pca2[:, 0]\n",
    "df_cleaned['PCA2'] = X_pca2[:, 1]\n",
    "df_cleaned['PCA3'] = X_pca2[:, 2] \n",
    "\n",
    "# Plotting the clusters using 3D scatter plot with Plotly\n",
    "fig = px.scatter_3d(df_cleaned, x='PCA1', y='PCA2', z='PCA3', color='cluster_label2', \n",
    "                    symbol='cluster_label2', opacity=0.7, \n",
    "                    labels={'PCA1': 'Principal Component 1', 'PCA2': 'Principal Component 2', 'PCA3': 'Principal Component 3'}, \n",
    "                    title='Beer Reviews Clustering (3D Scatter Plot)')\n",
    "fig.update_layout(scene=dict(xaxis_title='Principal Component 1', yaxis_title='Principal Component 2', zaxis_title='Principal Component 3'))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
